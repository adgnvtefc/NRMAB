Changelog

Week of 10/4

-added in active transition probabilities (no longer 100/0)
-refactored active transition into separate function
-implement node rearmament
-develop reward function for utility of current state
-reorganized certain functions into legacyMethods.py
-implemented recursive implementation of Bellman's equation

DO:
- implement tabular version of bellman
- prove submodularity for the Q(s,a) function assuming V(s) is given

Week of 10/11
-fixed issues from last week
-implemented q table learning
-refactored visual component

DO:
-performance comparison with some other techniques
 -hill-climbing (still need q table)
 -implement another representation of the Q table (ex. Q function can be represented by neural network)
 -no action
 -random action

-implement metrics for comparison
 -average number of active nodes
 -check submodularity

Week of 10/25:
-Deep Q learning attempted (it is horrendous)
-moved the hillclimbing functions into its separate class
-implemented performance comparison

DO:
-try using deep q learning using openai gym (or RLib, which is reinforcement learning with
 many algorithms implemented) (see slack)
-try to test the problem for larger sizes
 -find way to save tabular q learning
-more work on proof

Week of 11/1
-implemented Policy Network in place of Deep Q learning
-implemented metrics for large graphs (50+ nodes)
 -NOTE -- try large graphs with larger action values as well

DO:
-improve hill climbing without bellman equation
-implement deep-q network with hill climbing

Week of 11/8
DO: 
-make reward more complex
 -for deep q -- give it more look ahead to see if it can outperform hill climbing
 -give different nodes better reward values
-test in more resource sparse environments
-try just dqn, show dqn with hill climbing is better than traditional dqn
-collect stats -- cumulative activation over time and discounted activiation over time, percentage of time it outperforms x
